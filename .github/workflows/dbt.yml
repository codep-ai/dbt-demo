name: dbt
on:
  workflow_call:
    inputs:
      account_id:
        required: true
        type: string
      stage:
        required: true
        type: string
      env:
        required: true
        type: string
      env_name:
        required: true
        type: string
      bucket_name:
        required: true
        type: string
      dbt_bucket:
        required: true
        type: string
      SNOWFLAKE_ACCOUNT:
        required: true
        type: string
      SNOWFLAKE_USER:
        required: true
        type: string
      SNOWFLAKE_PASSWORD:
        required: true
        type: string
      SNOWFLAKE_ROLE:
        required: true
        type: string
      SNOWFLAKE_SCHEMA:
        equired: true
        type: string
      github_runner:
        required: true
        type: string
      pr_number:
        type: string
      slim_ci:
        type: string
    secrets:
      AWS_ACCESS_KEY_ID:
        required: true
      AWS_SECRET_ACCESS_KEY:
        required: true
env:
  env: ${{inputs.env}}
  STAGE: ${{inputs.stage}}
  ACCOUNT_ID: ${{inputs.account_id}}
  ACCOUNT_ENV: ${{inputs.env}}
  ENV_NAME: ${{inputs.env_name}}
  ACTION: deploy
  BUCKET_NAME: ${{inputs.bucket_name}}
  PRODUCT: apacdata
  REGION: ap-southeast-2
  SERVICE: data-pipeline
  TEAM_NAME: datapai
  TF_VAR_account_id: ${{inputs.account_id}}
  GITHUB_PR_NUMBER: ${{inputs.pr_number}}
jobs:
  dbt:
    name: dbt
    runs-on: ${{ fromJSON(inputs.github_runner) }}
    environment: ${{inputs.env}}
    timeout-minutes: 230
    steps:
      - uses: actions/checkout@v3
        with:
          persist-credentials: false
      - run: echo "/home/ubuntu/.local/bin" >> $GITHUB_PATH
      - run: echo "/usr/local/bin" >> $GITHUB_PATH
      - run: pip install -r requirements.txt
        working-directory: .
      # - run: pip install psycopg2 pyyaml psycopg2-binary
      - uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-southeast-2
          role-to-assume: arn:aws:iam::${{ env.TF_VAR_account_id }}:role/${{ env.TF_VAR_role_name }}
          role-duration-seconds: 7200
      - run: aws s3 cp ${{ inputs.dbt_bucket }}target/manifest.json .
        # --exclude 'dbt_packages/*'
        working-directory: .
      # - if: ${{ inputs.stage != 'ci' }}
      #   run: python3 save_delta.py
      - run: dbt deps --profiles-dir . --target ${{inputs.stage}}
        working-directory: .
      - name: check source freshness
        run: dbt source freshness --profiles-dir . --target ${{inputs.stage}}
        if: ${{ inputs.env == 'prod' }}
        working-directory: .

        # iterate through all source nodes, create if missing, refresh metadata
      - run: dbt run-operation stage_external_sources --profiles-dir . --target ${{inputs.stage}}
        working-directory: .

      - name: dbt slim CI build
        if: ${{ inputs.slim_ci == 'true' || inputs.stage == 'ci' }}
        run: dbt build --profiles-dir . --target ${{inputs.stage}}  --select "@state:modified" --defer --state .
        working-directory: .


      - name: dbt Full CI build
        if: ${{ inputs.slim_ci != 'true' && inputs.stage != 'ci'  }}
        run: dbt build --profiles-dir . --target ${{inputs.stage}} 
        working-directory: dbt

      - run: dbt docs generate --profiles-dir . --target ${{inputs.stage}}
        if: ${{ inputs.stage == 'prod' }}
        working-directory: .

      - run: aws s3 sync . ${{ inputs.dbt_bucket }} --size-only --delete --exclude 'codeartifact.txt' --exclude 'static/*'
        working-directory: .
      - name: Publish Docs ðŸš€
        if: ${{ inputs.stage == 'prod' }}
        uses: JamesIves/github-pages-deploy-action@v4.4.3
        with:
          branch: gh-pages
          folder: ./target
  airflow:
    name: Airflow
    runs-on: ubuntu-latest
    environment: ${{inputs.env}}
    timeout-minutes: 40
    steps:
      - uses: actions/checkout@v3
        with:
          persist-credentials: false
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.8"
      # - name: Install dependencies
      #   run: |
      #     pip install -r requirements/requirements.txt
      #     pip install black flake8 sqlfluff black pytest flake8 apache-airflow==2.4.3
      #     pip check
      #   working-directory: airflow
      # - name: Lint with Flake8
      #   run: flake8 --ignore E501 dags --benchmark -v
      #   working-directory: airflow
      # - name: Black Diff
      #   run: black airflow/dags --diff
      # - name: Confirm Black code compliance (psf/black)
      #   run: black airflow/dags --check
      # - run: pytest -v
      # - uses: aws-actions/configure-aws-credentials@v3
      #   with:
      #     aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
      #     aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      #     aws-region: ap-southeast-2
      #     role-to-assume: arn:aws:iam::${{ env.TF_VAR_account_id }}:role/${{ env.TF_VAR_role_name }}
      #     role-duration-seconds: 7200
      # - name: upload airflow
      #   run: aws s3 sync airflow s3://${{ env.BUCKET_NAME }} --size-only --exclude 'dags/dbt/*' --exclude 'plugins/*' --exclude 'requirements/*' --exclude 'dags/codeartifact.txt' --delete --exclude 'static/*'
      # - run: aws mwaa get-environment --name ${{inputs.stage}}-airflow
      # - run: echo $(aws mwaa create-web-login-token --name ${{inputs.stage}}-airflow ) | sed 's/./& /g'
      # - run: python invoke_airflow.py
